# Claude SDK Search Project

A Python project that demonstrates using the Claude Agent SDK to answer questions by searching through documents, with automated evaluation capabilities.

## Overview

This project provides a complete pipeline for:
1. **Question Processing**: Using Claude Agent SDK to answer questions by searching through source documents
2. **Answer Evaluation**: Automated evaluation of Claude's answers against expected answers using OpenAI's GPT models as judges
3. **Batch Processing**: Process multiple questions from JSON files and generate comprehensive evaluation reports

## Features

- üîç **Document Search**: Uses Claude Agent SDK with file tools (Read, Write, Grep, Glob) to search through documents
- üìä **Automated Evaluation**: Evaluates answers on accuracy and completeness using OpenAI's structured output
- üìà **Batch Processing**: Process multiple questions from JSON files
- üí∞ **Cost Tracking**: Monitor Claude API usage and costs
- üìã **Detailed Reports**: Generate comprehensive evaluation reports with individual and aggregate scores

## Prerequisites

- Python 3.14 or higher
- Claude API key
- OpenAI API key (for evaluation)

## Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd claude-sdk-search
   ```

2. **Install dependencies using uv** (recommended):
   ```bash
   uv sync
   ```

   Or using pip:
   ```bash
   pip install -e .
   ```

3. **Set up environment variables**:
   Create a `.env` file in the project root:
   ```bash
   # Required for Claude Agent SDK
   CLAUDE_API_KEY=your_claude_api_key_here
   
   # Required for evaluation
   OPENAI_API_KEY=your_openai_api_key_here
   ```

## Usage

### 1. Basic Question Answering

Process a single question against a source document:

```bash
python file_query.py -i data/questions_answers.json -q data/2507.13334v2.md
```

**Parameters**:
- `-i, --input-file`: JSON file containing questions and expected answers
- `-q, --questions-file`: Source file to query for answers

### 2. Answer Evaluation

Evaluate Claude's answers against expected answers:

```bash
python evaluate_answers.py -i data/questions_answers_results.json
```

**Parameters**:
- `-i, --input-file`: JSON file with question-answer data (generated by file_query.py)
- `-o, --output-file`: Output file for evaluation results (optional, auto-generated if not specified)
- `-k, --api-key`: OpenAI API key (optional if OPENAI_API_KEY env var is set)

### 3. Complete Workflow

Run the complete pipeline:

```bash
# Step 1: Generate answers
python file_query.py -i data/questions_answers.json -q data/2507.13334v2.md

# Step 2: Evaluate answers
python evaluate_answers.py -i data/questions_answers_results.json
```

## Data Format

### Input Questions File (`questions_answers.json`)

```json
[
  {
    "question": "What foundational shift in AI necessitated the evolution of prompt engineering?",
    "answer": "The necessity arose because Large Language Models (LLMs) have evolved from simple instruction-following systems into the core reasoning engines of complex applications..."
  }
]
```

### Results File (`questions_answers_results.json`)

Generated by `file_query.py`:

```json
[
  {
    "question": "What foundational shift in AI necessitated the evolution of prompt engineering?",
    "expected_answer": "The necessity arose because Large Language Models (LLMs) have evolved...",
    "claude_answer": "Based on the document, the foundational shift occurred when LLMs evolved..."
  }
]
```

### Evaluation Results

Generated by `evaluate_answers.py`:

```json
[
  {
    "question": "What foundational shift in AI necessitated the evolution of prompt engineering?",
    "claude_answer": "Based on the document, the foundational shift occurred when LLMs evolved...",
    "expected_answer": "The necessity arose because Large Language Models (LLMs) have evolved...",
    "evaluation": {
      "accuracy_score": 0.85,
      "completeness_score": 0.90,
      "reasoning": "The answer correctly identifies the key shift but misses some specific details..."
    }
  }
]
```

## Project Structure

```
claude-sdk-search/
‚îú‚îÄ‚îÄ data/                          # Data files
‚îÇ   ‚îú‚îÄ‚îÄ 2507.13334v2.md          # Source document (research paper)
‚îÇ   ‚îú‚îÄ‚îÄ questions_answers.json    # Input questions and expected answers
‚îÇ   ‚îî‚îÄ‚îÄ questions_answers_short.json # Shorter version for testing
‚îú‚îÄ‚îÄ file_query.py                 # Main script for question answering
‚îú‚îÄ‚îÄ evaluate_answers.py           # Answer evaluation script
‚îú‚îÄ‚îÄ pyproject.toml               # Project dependencies
‚îú‚îÄ‚îÄ uv.lock                      # Dependency lock file
‚îî‚îÄ‚îÄ README.md                    # This file
```

## Configuration

### Claude Agent SDK Options

The project uses these Claude Agent SDK settings:
- **Model**: `sonnet`
- **Allowed Tools**: `["Read", "Write", "Grep", "Glob"]`
- **System Prompt**: "You are a helpful file assistant. You must use tools to answer the question."

### Evaluation Settings

- **Evaluation Model**: `gpt-5-mini` (OpenAI)
- **Evaluation Criteria**: Accuracy (0.0-1.0) and Completeness (0.0-1.0)
- **Structured Output**: Uses Pydantic models for consistent evaluation results

## Output Examples

### Question Processing Output

```
============================================================
Processing question 1/10
Question: What foundational shift in AI necessitated the evolution of prompt engineering?
============================================================

üîß Tool Use: Read
   Input: {"path": "data/2507.13334v2.md"}
   Result: [Document content...]

Claude: Based on the document, the foundational shift occurred when Large Language Models evolved from simple instruction-following systems into core reasoning engines of complex applications...

Cost: $0.0234
```

### Evaluation Output

```
============================================================
EVALUATION SUMMARY
============================================================
Total Questions: 10
Average Accuracy Score: 0.847
Average Completeness Score: 0.891
Min Accuracy Score: 0.650
Max Accuracy Score: 0.950
Min Completeness Score: 0.720
Max Completeness Score: 0.980
============================================================
```
